{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Modeling Notebook\n",
    "Welcome to the modeling notebook"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83e13bb2b2354f45"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from plotly.figure_factory._dendrogram import sch\n",
    "from sklearn.cluster import KMeans, OPTICS\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import clone\n",
    "import umap\n",
    "from scipy.stats import ttest_ind, probplot, shapiro\n",
    "import statsmodels.stats.api as sms\n",
    "from pycm import ConfusionMatrix\n",
    "import joblib\n",
    "import os\n",
    "import pickle\n",
    "#from keras.src.applications.mobilenet_v2 import MobileNetV2\n",
    "#from keras.src.callbacks import EarlyStopping\n",
    "#from keras.src.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Dense, ZeroPadding2D, Conv1D, MaxPooling1D, Flatten, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3387997d219be23",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    Generic Class responsible for loading the dataset\n",
    "\n",
    "    Attributes:\n",
    "        filename (str): The filename of the dataset to load.\n",
    "        target (str): The target of the dataset to load.\n",
    "\n",
    "    Attributes (after loading the data):\n",
    "        data (DataFrame): The main dataset containing both features and target variable.\n",
    "        labels (DataFrame): The target variable.\n",
    "        numerical_features (List): List of numerical features in the dataset.\n",
    "        categorical_features (List): List of categorical features in the dataset.\n",
    "\n",
    "\n",
    "    Methods:\n",
    "        _load_data(): Loads the dataset,and assigns the data and labels to the appropriate attributes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename, target):\n",
    "        \"\"\"\n",
    "        Initializes the DataLoader with the filename of the dataset.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The filename of the dataset to load.\n",
    "            target (str): The target of the dataset to load.\n",
    "        \"\"\"\n",
    "        self.filename = filename\n",
    "\n",
    "        self.data = None\n",
    "        self.target = target\n",
    "        self.labels = None\n",
    "        self.numerical_features = []\n",
    "        self.categorical_features = []\n",
    "\n",
    "        # Load data\n",
    "        self._load_data(target)\n",
    "\n",
    "    def _load_data(self, target):\n",
    "        \"\"\"\n",
    "        Loads the dataset from the specified filename,\n",
    "        and assigns the data and labels to the appropriate attributes.\n",
    "\n",
    "        Args:\n",
    "            target (str): The target of the dataset to load.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load the dataset\n",
    "            self.data = pd.read_csv(self.filename)\n",
    "\n",
    "            # Validate if the target column exists in the dataset\n",
    "            if target not in self.data.columns:\n",
    "                raise ValueError(f\"Target column '{target}' not found in the dataset.\")\n",
    "\n",
    "            self.labels = self.data[target]\n",
    "\n",
    "            print(\"Data loaded successfully.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(\"File not found. Please check the file path.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e858ce4eea8a4b0a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DataManipulator(DataLoader):\n",
    "    \"\"\"\n",
    "    A class for manipulating data loaded from a file.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The path to the data file.\n",
    "        target (str): The target variable in the data.\n",
    "\n",
    "    Attributes:\n",
    "        data (DataFrame): The loaded data.\n",
    "\n",
    "    Methods:\n",
    "        _describe_variables: Prints information about the data, including data info, unique values, and statistical distribution.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified file is not found.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename, target):\n",
    "        \"\"\"\n",
    "        Initialize the class with a filename and target variable.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The path to the file.\n",
    "            target (str): The name of the target variable.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the file is not found.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            super().__init__(filename, target)\n",
    "            print(\"\\nData Description:\")\n",
    "            self._describe_variables()\n",
    "        except FileNotFoundError:\n",
    "            print(\"File not found. Please check the file path.\")\n",
    "\n",
    "    def _describe_variables(self):\n",
    "        \"\"\"\n",
    "        Prints information about the data, including data info, unique values, and statistical distribution.\n",
    "        \"\"\"\n",
    "        print(\"\\nInformation of Data:\")\n",
    "        print(self.data.info())\n",
    "\n",
    "        print(\"\\nUnique values of features:\")\n",
    "        print(self.data.nunique())\n",
    "\n",
    "        print(\"\\nStatistical distribution of each variable:\")\n",
    "        print(self.data.describe())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3fb74453dbf0493",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DataPreProcessing:\n",
    "    \"\"\"\n",
    "    Class for performing data preprocessing tasks, mostly encoding.\n",
    "\n",
    "    Args:\n",
    "        data_loader (DataLoader): The DataLoader object containing the dataset.\n",
    "\n",
    "    Attributes:\n",
    "        data_loader (DataLoader): The DataLoader object containing the dataset.\n",
    "\n",
    "    Methods:\n",
    "        _sanity_check(): Performs a sanity check on the DataLoader object.\n",
    "        _determine_range(): Displays the range of values for each variable without considering the class label.\n",
    "        _age_encode(): Encodes the AgeCategory variable into numerical values.\n",
    "        _encode_numerical_values(column, mapping): Encodes a variable into numerical values using the provided mapping.\n",
    "        _encode_data(): Encodes categorical features into numerical values and fills numerical and categorical features arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_loader):\n",
    "        \"\"\"\n",
    "        Initializes an instance of the class.\n",
    "\n",
    "        Args:\n",
    "            data_loader: The data loader object used to load the data.\n",
    "        \"\"\"\n",
    "        self.data_loader = data_loader\n",
    "\n",
    "        self._sanity_check()\n",
    "\n",
    "        self._encode_data()\n",
    "\n",
    "        self._determine_range()\n",
    "\n",
    "    def _sanity_check(self):\n",
    "        \"\"\"\n",
    "        Performs a sanity check on the DataLoader object.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the DataLoader object is not provided or is not a pandas DataFrame.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not self.data_loader:\n",
    "                raise ValueError(\"DataLoader object is not provided.\")\n",
    "            if not isinstance(self.data_loader.data, pd.DataFrame):\n",
    "                raise ValueError(\"Invalid DataLoader object. It should contain a pandas DataFrame.\")\n",
    "        except Exception as error:\n",
    "            print(f\"Error occurred: {error}\")\n",
    "            return False\n",
    "\n",
    "    def _determine_range(self):\n",
    "        \"\"\"\n",
    "        Displays the range of values for each variable without considering the class label.\n",
    "        \"\"\"\n",
    "        print(\"\\nRange of values for each variable:\")\n",
    "        print(self.data_loader.data.drop(columns=[\"HeartDisease\"]).max() - self.data_loader.data.drop(\n",
    "            columns=[\"HeartDisease\"]).min())\n",
    "\n",
    "    def _age_encode(self):\n",
    "        \"\"\"\n",
    "        Encodes AgeCategory into numerical values.\n",
    "        \"\"\"\n",
    "        age_map = {\n",
    "            \"18-24\": 1, \"25-29\": 2, \"30-34\": 3, \"35-39\": 4,\n",
    "            \"40-44\": 5, \"45-49\": 6, \"50-54\": 7, \"55-59\": 8,\n",
    "            \"60-64\": 9, \"65-69\": 10, \"70-74\": 11, \"75-79\": 12,\n",
    "            \"80 or older\": 13\n",
    "        }\n",
    "        self.data_loader.data[\"AgeCategory\"] = self.data_loader.data[\"AgeCategory\"].map(age_map)\n",
    "\n",
    "    def _encode_numerical_values(self, column, mapping):\n",
    "        \"\"\"\n",
    "        Encodes a variable into numerical values using the provided mapping.\n",
    "\n",
    "        Args:\n",
    "            column (str): The name of the column to be encoded.\n",
    "            mapping (dict): The mapping of categorical values to numerical values.\n",
    "        \"\"\"\n",
    "        self.data_loader.data[column] = self.data_loader.data[column].map(mapping)\n",
    "\n",
    "    def _encode_data(self):\n",
    "        \"\"\"\n",
    "        Encodes categorical features into numerical values and fills numerical and categorical features arrays.\n",
    "        \"\"\"\n",
    "        # Map categorical features to numerical values\n",
    "        categorical_mappings = {\n",
    "            \"HeartDisease\": {\"No\": 0, \"Yes\": 1},\n",
    "            \"Smoking\": {\"No\": 0, \"Yes\": 1},\n",
    "            \"AlcoholDrinking\": {\"No\": 0, \"Yes\": 1},\n",
    "            \"Stroke\": {\"No\": 0, \"Yes\": 1},\n",
    "            \"DiffWalking\": {\"No\": 0, \"Yes\": 1},\n",
    "            \"Sex\": {\"Female\": 0, \"Male\": 1},\n",
    "            \"Diabetic\": {\"No\": 0, \"No, borderline diabetes\": 0, \"Yes (during pregnancy)\": 1, \"Yes\": 1},\n",
    "            \"PhysicalActivity\": {\"No\": 0, \"Yes\": 1},\n",
    "            \"Asthma\": {\"No\": 0, \"Yes\": 1},\n",
    "            \"KidneyDisease\": {\"No\": 0, \"Yes\": 1},\n",
    "            \"SkinCancer\": {\"No\": 0, \"Yes\": 1}\n",
    "        }\n",
    "        for column, mapping in categorical_mappings.items():\n",
    "            self._encode_numerical_values(column, mapping)\n",
    "\n",
    "        # Encode AgeCategory, Race, and GenHealth\n",
    "        self._age_encode()\n",
    "        self._encode_numerical_values(\"Race\", {\"White\": 1, \"Black\": 2, \"Hispanic\": 3, \"Asian\": 4,\n",
    "                                               \"American Indian/Alaskan Native\": 5, \"Other\": 6})\n",
    "        self._encode_numerical_values(\"GenHealth\", {\"Excellent\": 5, \"Very good\": 4, \"Good\": 3, \"Fair\": 2, \"Poor\": 1})\n",
    "\n",
    "        # Fill the numerical and categorical features arrays\n",
    "        self.data_loader.categorical_features.append(\"HeartDisease\")\n",
    "        self.data_loader.categorical_features.append(\"Smoking\")\n",
    "        self.data_loader.categorical_features.append(\"AlcoholDrinking\")\n",
    "        self.data_loader.categorical_features.append(\"Stroke\")\n",
    "        self.data_loader.categorical_features.append(\"DiffWalking\")\n",
    "        self.data_loader.categorical_features.append(\"Sex\")\n",
    "        self.data_loader.categorical_features.append(\"Race\")\n",
    "        self.data_loader.categorical_features.append(\"Diabetic\")\n",
    "        self.data_loader.categorical_features.append(\"PhysicalActivity\")\n",
    "        self.data_loader.categorical_features.append(\"GenHealth\")\n",
    "        self.data_loader.categorical_features.append(\"Asthma\")\n",
    "        self.data_loader.categorical_features.append(\"KidneyDisease\")\n",
    "        self.data_loader.categorical_features.append(\"SkinCancer\")\n",
    "\n",
    "        self.data_loader.numerical_features.append(\"BMI\")\n",
    "        self.data_loader.numerical_features.append(\"PhysicalHealth\")\n",
    "        self.data_loader.numerical_features.append(\"MentalHealth\")\n",
    "        self.data_loader.numerical_features.append(\"AgeCategory\")\n",
    "        self.data_loader.numerical_features.append(\"SleepTime\")\n",
    "\n",
    "        print(\"\\nProcessed Dataset:\")\n",
    "        print(self.data_loader.data.info())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94171bd4484c0836",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DataCleaning:\n",
    "    \"\"\"\n",
    "    A class for performing data cleaning operations on a dataset.\n",
    "\n",
    "    Args:\n",
    "        data_loader (DataLoader): An instance of the DataLoader class that provides access to the dataset.\n",
    "\n",
    "    Attributes:\n",
    "        data_loader (DataLoader): An instance of the DataLoader class that provides access to the dataset.\n",
    "\n",
    "    Methods:\n",
    "        handle_missing_values: Removes rows with missing values from the dataset.\n",
    "        remove_duplicates: Removes duplicate rows from the dataset.\n",
    "        detect_and_remove_outliers: Detects and removes outliers from the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_loader):\n",
    "        \"\"\"\n",
    "        Initializes an instance of the class.\n",
    "\n",
    "        Args:\n",
    "            data_loader: The data loader object used to load the dataset.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.data_loader = data_loader\n",
    "        print(\"\\nOriginal Dataset before cleaning:\")\n",
    "        print(self.data_loader.data.info())\n",
    "\n",
    "    def handle_missing_values(self):\n",
    "        \"\"\"\n",
    "        This method checks for missing values in the dataset and removes any rows that contain missing values.\n",
    "        It prints the number of missing values for each column before and after removing the rows.\n",
    "        If there are no missing values, no rows are removed.\n",
    "        \"\"\"\n",
    "        print(\"Missing values:\\n\", self.data_loader.data.isnull().sum())\n",
    "\n",
    "        if self.data_loader.data.isnull().sum().sum() > 0:\n",
    "            self.data_loader.data = self.data_loader.data.dropna()\n",
    "\n",
    "    def remove_duplicates(self):\n",
    "        \"\"\"\n",
    "            This method checks for duplicate rows in the dataset and removes them if any are found.\n",
    "            It prints the number of duplicate rows before and after the removal process.\n",
    "            \"\"\"\n",
    "        print(\"Duplicate Rows:\", self.data_loader.data.duplicated().sum())\n",
    "\n",
    "        if self.data_loader.data.duplicated().sum() > 0:\n",
    "            self.data_loader.data = self.data_loader.data.drop_duplicates(keep='first')\n",
    "\n",
    "    def detect_and_remove_outliers(self):\n",
    "        \"\"\"\n",
    "        This method iterates over each feature in the dataset and detects outliers using the interquartile range (IQR) method.\n",
    "        Outliers are defined as values that fall below the lower bound (Q1 - 1.5 * IQR) or above the upper bound (Q3 + 1.5 * IQR).\n",
    "        Outliers are then removed from the dataset.\n",
    "\n",
    "        If a feature has only two unique values, it is skipped as it is not suitable for outlier detection.\n",
    "        After removing outliers, if a feature has only one unique value, it is considered redundant and is deleted from the dataset.\n",
    "        \"\"\"\n",
    "        print(\"\\nDetecting outliers:\")\n",
    "        features_to_delete = []\n",
    "        for feature in self.data_loader.data.columns:\n",
    "            # Skip features with only two unique values\n",
    "            if len(self.data_loader.data[feature].unique()) == 2:\n",
    "                print(f\"Skipping '{feature}' as it has only two unique values.\")\n",
    "                continue\n",
    "\n",
    "            q1 = self.data_loader.data[feature].quantile(0.25)\n",
    "            q3 = self.data_loader.data[feature].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            outliers_indices = self.data_loader.data[\n",
    "                (self.data_loader.data[feature] < lower_bound) | (self.data_loader.data[feature] > upper_bound)].index\n",
    "\n",
    "            print(f\"Outliers in '{feature}'.\" if not outliers_indices.empty else f\"No outliers in '{feature}'.\")\n",
    "\n",
    "            self.data_loader.data.drop(outliers_indices, inplace=True)\n",
    "\n",
    "            # Verify if the feature after removing outliers has only one unique value\n",
    "            if len(self.data_loader.data[feature].unique()) == 1:\n",
    "                print(f\"Feature '{feature}' has only one unique value after removing outliers. Deleting it.\")\n",
    "                features_to_delete.append(feature)\n",
    "\n",
    "        # Remove features with only one unique value\n",
    "        self.data_loader.data.drop(columns=features_to_delete, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6a5a8e40f432d42",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DataVisualization:\n",
    "    \"\"\"\n",
    "    A class for visualizing data using various plot types.\n",
    "    \n",
    "    Args:\n",
    "        data_loader (DataLoader): A DataLoader object that provides access to the data.\n",
    "        valid_plot_types (list): A list of valid plot types that can be used for visualization.\n",
    "    \n",
    "    Attributes:\n",
    "        data_loader (DataLoader): A DataLoader object that provides access to the data.\n",
    "        valid_plot_types (list): A list of valid plot types that can be used for visualization.\n",
    "        labels (list): A list of unique labels in the dataset.\n",
    "    \n",
    "    Methods:\n",
    "        plot_all_features(): Plots histograms for all features in the dataset.\n",
    "        plots(plot_types): Plots the specified types of plots for each feature in the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_loader, valid_plot_types):\n",
    "        \"\"\"\n",
    "        Initializes the DataVisualization class with a DataLoader object.\n",
    "\n",
    "        Parameters:\n",
    "        - data_loader (DataLoader): The DataLoader object used to load the data.\n",
    "        - valid_plot_types (list): A list of valid plot types that can be used for visualization.\n",
    "\n",
    "        Attributes:\n",
    "        - data_loader (DataLoader): The DataLoader object used to load the data.\n",
    "        - valid_plot_types (list): A list of valid plot types that can be used for visualization.\n",
    "        - labels (list): A list of unique labels in the loaded data.\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_loader = data_loader\n",
    "        self.valid_plot_types = valid_plot_types\n",
    "        self.labels = self.data_loader.data[self.data_loader.target].unique().tolist()\n",
    "\n",
    "    def plot_all_features(self):\n",
    "        \"\"\"\n",
    "        Plots histograms for all features in the dataset.\n",
    "\n",
    "        This method generates a histogram for each feature in the dataset. The histograms show the frequency distribution\n",
    "        of values for each feature. If labels are provided, multiple histograms will be plotted for each feature, one for\n",
    "        each label.\n",
    "        \"\"\"\n",
    "        num_features = len(self.data_loader.data.columns.tolist())\n",
    "        num_cols = 3  # Adjust the number of columns to control subplot arrangement\n",
    "        num_rows = int(np.ceil(num_features / num_cols))\n",
    "\n",
    "        fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 3 * num_rows))\n",
    "\n",
    "        for idx, ax in enumerate(axes.flat):\n",
    "            if idx < num_features:\n",
    "                ax.set_title(f'Feature {self.data_loader.data.columns.tolist()[idx]}', fontsize=12)\n",
    "                ax.set_xlabel('Value', fontsize=10)\n",
    "                ax.set_ylabel('Frequency', fontsize=10)\n",
    "                ax.grid(True)\n",
    "\n",
    "                if self.labels is not None:\n",
    "                    # Add a plot per feature and label\n",
    "                    for label in self.labels:\n",
    "                        mask = np.array(self.data_loader.data[self.data_loader.target] == label)\n",
    "                        ax.hist(self.data_loader.data.loc[mask, self.data_loader.data.columns.tolist()[idx]], bins=20,\n",
    "                                alpha=0.7, label=label)\n",
    "                    ax.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plots(self, plot_types):\n",
    "        \"\"\"\n",
    "        Plots the specified types of plots for each feature in the dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        - plot_types (list): A list of plot types to be plotted.\n",
    "        \"\"\"\n",
    "        for plot_type in plot_types:\n",
    "            # Check if the selected plots are in the list of available plots\n",
    "            if plot_type not in self.valid_plot_types:\n",
    "                print(\n",
    "                    f\"Ignoring invalid plot type: {plot_type}. Supported plot types: {', '.join(self.valid_plot_types)}\")\n",
    "                continue\n",
    "\n",
    "            for feature in self.data_loader.data.columns:\n",
    "                # Create a figure with a single subplot for each feature\n",
    "                if plot_type == 'box' and self.data_loader.data[feature].nunique() > 2:\n",
    "                    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "                    sns.boxplot(x=self.data_loader.target, y=feature, data=self.data_loader.data, ax=ax)\n",
    "                    ax.set_title(f'Boxplot of {feature} by {self.data_loader.target}')\n",
    "\n",
    "                    # Set the tick labels on the x-axis to \"No\" and \"Yes\"\n",
    "                    ax.set_xticklabels(['No', 'Yes'])\n",
    "\n",
    "                    plt.show()\n",
    "\n",
    "        if 'correlation' in plot_types:\n",
    "            correlation = self.data_loader.data.corr().round(2)\n",
    "            heartdisease_correlation = correlation['HeartDisease'].sort_values(ascending=False)\n",
    "\n",
    "            plt.figure(figsize=(15, 12))\n",
    "            sns.heatmap(correlation, annot=True, cmap='YlOrBr', annot_kws={'size': 8})\n",
    "            plt.title('Correlation Heatmap')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            print(\"\\nCorrelation of the features with Heart Disease:\\n\")\n",
    "            print(heartdisease_correlation)\n",
    "\n",
    "        if 'barh' in plot_types:\n",
    "            # Train a RandomForestClassifier model\n",
    "            clf = RandomForestClassifier()\n",
    "            X = self.data_loader.data.drop(columns=[self.data_loader.target])  # Features\n",
    "            y = self.data_loader.data[self.data_loader.target]  # Target variable\n",
    "            clf.fit(X, y)\n",
    "\n",
    "            # Calculate permutation importance\n",
    "            result = permutation_importance(clf, X, y, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "            perm_sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "            # Visualize feature importance\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.barplot(x=result.importances_mean[perm_sorted_idx], y=X.columns[perm_sorted_idx], color='blue')\n",
    "            plt.xlabel('Permutation Importance')\n",
    "            plt.ylabel('Features')\n",
    "            plt.title('Permutation Importance')\n",
    "            plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4872ad6c928520",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DimensionalityReduction:\n",
    "    \"\"\"\n",
    "    Class for performing dimensionality reduction techniques such as PCA and UMAP.\n",
    "\n",
    "    Args:\n",
    "        data_loader (DataLoader): An instance of the DataLoader class that provides the data.\n",
    "\n",
    "    Attributes:\n",
    "        data_loader (DataLoader): An instance of the DataLoader class that provides the data.\n",
    "        dataset (DataFrame): A sample of 30% of the data.\n",
    "        data (nparray): The standardized data.\n",
    "        target (Series): The target variable from the data.\n",
    "\n",
    "    Methods:\n",
    "        plot_projection(projection, title): Plot the projection of the data.\n",
    "        compute_pca(n_components): Perform Principal Component Analysis (PCA) on the data.\n",
    "        compute_umap(n_components, n_neighbors, min_dist, metric): Perform Uniform Manifold Approximation and Projection (UMAP) on the data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_loader):\n",
    "        \"\"\"\n",
    "        Initializes an instance of MyClass.\n",
    "\n",
    "        Parameters:\n",
    "        - data_loader (DataLoader): An object that loads the data.\n",
    "\n",
    "        Attributes:\n",
    "        - data_loader: The data loader object.\n",
    "        - dataset: A sample of 30% of the data.\n",
    "        - data: The standardized data.\n",
    "        - target: The target variable from the data.\n",
    "        \"\"\"\n",
    "        self.data_loader = data_loader\n",
    "\n",
    "        # Sample 30% of the data\n",
    "        self.dataset = self.data_loader.data.sample(frac=0.3, random_state=42)\n",
    "\n",
    "        self.data = StandardScaler().fit_transform(self.data_loader.data.drop(columns=[self.data_loader.target]))\n",
    "        self.target = self.data_loader.data[self.data_loader.target]\n",
    "\n",
    "    def plot_projection(self, projection, title):\n",
    "        \"\"\"\n",
    "        Plot the projection of the data.\n",
    "\n",
    "        Args:\n",
    "        - projection: The projected data.\n",
    "        - title (str): The title of the plot.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        if projection.shape[1] == 1:\n",
    "            plt.scatter(projection, np.zeros_like(projection), c=self.target, alpha=0.5, cmap='viridis')\n",
    "        else:\n",
    "            plt.scatter(projection[:, 0], projection[:, 1], c=self.target, alpha=0.5, cmap='viridis')\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Component 1')\n",
    "        plt.ylabel('Component 2')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def compute_pca(self, n_components=2):\n",
    "        \"\"\"\n",
    "        Perform Principal Component Analysis (PCA) on the data.\n",
    "\n",
    "        Args:\n",
    "        - n_components: The number of components to keep.\n",
    "\n",
    "        Returns:\n",
    "        - The projected data after PCA.\n",
    "        \"\"\"\n",
    "        return PCA(n_components=n_components).fit_transform(self.data)\n",
    "\n",
    "    def compute_umap(self, n_components=2, n_neighbors=8, min_dist=0.5, metric='euclidean'):\n",
    "        \"\"\"\n",
    "        Perform Uniform Manifold Approximation and Projection (UMAP) on the data.\n",
    "\n",
    "        Args:\n",
    "        - n_components: The number of components to keep.\n",
    "        - n_neighbors: The number of neighbors to consider for each point.\n",
    "        - min_dist: The minimum distance between points in the low-dimensional representation.\n",
    "        - metric: The distance metric to use.\n",
    "\n",
    "        Returns:\n",
    "        - The projected data after UMAP.\n",
    "        \"\"\"\n",
    "        return umap.UMAP(n_components=n_components, n_neighbors=n_neighbors, min_dist=min_dist,\n",
    "                         metric=metric).fit_transform(self.data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3448de6cdf89824",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class HypothesisTester:\n",
    "    \"\"\"\n",
    "    Class for performing hypothesis tests and generating Q-Q plots.\n",
    "\n",
    "    Parameters:\n",
    "    - data_loader: An instance of the DataLoader class used for loading the data.\n",
    "\n",
    "    Attributes:\n",
    "    - data_loader: An instance of the DataLoader class used for loading the data.\n",
    "    - BMI_with_HD: Column data for BMI with Heart Disease.\n",
    "    - Smoke_with_HD: Column data for Smoking with Heart Disease.\n",
    "    - Alcohol_with_HD: Column data for Alcohol Drinking with Heart Disease.\n",
    "    - Stroke_with_HD: Column data for Stroke with Heart Disease.\n",
    "    - PH_with_HD: Column data for Physical Health with Heart Disease.\n",
    "    - MH_with_HD: Column data for Mental Health with Heart Disease.\n",
    "    - DW_with_HD: Column data for Diff Walking with Heart Disease.\n",
    "    - Sex_with_HD: Column data for Sex with Heart Disease.\n",
    "    - AC_with_HD: Column data for Age Category with Heart Disease.\n",
    "    - Diabetic_with_HD: Column data for Diabetic with Heart Disease.\n",
    "    - PA_with_HD: Column data for Physical Activity with Heart Disease.\n",
    "    - GH_with_HD: Column data for Gen Health with Heart Disease.\n",
    "    - ST_with_HD: Column data for Sleep Time with Heart Disease.\n",
    "    - Asthma_with_HD: Column data for Asthma with Heart Disease.\n",
    "    - KD_with_HD: Column data for Kidney Disease with Heart Disease.\n",
    "    - SC_with_HD: Column data for Skin Cancer with Heart Disease.\n",
    "    - BMI_without_HD: Column data for BMI without Heart Disease.\n",
    "    - Smoke_without_HD: Column data for Smoking without Heart Disease.\n",
    "    - Alcohol_without_HD: Column data for Alcohol Drinking without Heart Disease.\n",
    "    - Stroke_without_HD: Column data for Stroke without Heart Disease.\n",
    "    - PH_without_HD: Column data for Physical Health without Heart Disease.\n",
    "    - MH_without_HD: Column data for Mental Health without Heart Disease.\n",
    "    - DW_without_HD: Column data for Diff Walking without Heart Disease.\n",
    "    - Sex_without_HD: Column data for Sex without Heart Disease.\n",
    "    - AC_without_HD: Column data for Age Category without Heart Disease.\n",
    "    - Diabetic_without_HD: Column data for Diabetic without Heart Disease.\n",
    "    - PA_without_HD: Column data for Physical Activity without Heart Disease.\n",
    "    - GH_without_HD: Column data for Gen Health without Heart Disease.\n",
    "    - ST_without_HD: Column data for Sleep Time without Heart Disease.\n",
    "    - Asthma_without_HD: Column data for Asthma without Heart Disease.\n",
    "    - KD_without_HD: Column data for Kidney Disease without Heart Disease.\n",
    "    - SC_without_HD: Column data for Skin Cancer without Heart Disease.\n",
    "    - variable_names: List of variable names.\n",
    "    - data_samples: Tuple of data samples.\n",
    "    - normal_distributed_variables_with_HD: List of normal distributed variables with Heart Disease.\n",
    "    - normal_distributed_variables_without_HD: List of normal distributed variables without Heart Disease.\n",
    "    - not_normal_distributed_variables_with_HD: List of not normal distributed variables with Heart Disease.\n",
    "    - not_normal_distributed_variables_without_HD: List of not normal distributed variables without Heart Disease.\n",
    "\n",
    "    Methods:\n",
    "    - _wilcoxon_ranksum_test(self, group1, group2): Perform Wilcoxon rank-sum test (Mann-Whitney U test) for two independent samples.\n",
    "    - _unpaired_t_test(self, group1, group2): Perform unpaired t-test for two independent samples.\n",
    "    - perform_tests(self): Perform hypothesis tests for all variable pairs.\n",
    "    - qq_plots(self, distribution='norm'): Generate Q-Q plots for all variables.\n",
    "    - test_normality(self): Test the normality assumption for all variables.\n",
    "    - distribute_normality_data(self): Distribute data based on normality assumption.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_loader):\n",
    "        \"\"\"\n",
    "        Initialize the HypothesisTester object.\n",
    "\n",
    "        Parameters:\n",
    "        - data_loader: An instance of the DataLoader class used for loading the data.\n",
    "        \"\"\"\n",
    "        self.data_loader = data_loader\n",
    "\n",
    "        # Column Data with Hearth Disease\n",
    "        self.BMI_with_HD = self.data_loader.data['BMI'][self.data_loader.data['HeartDisease'] == 1]\n",
    "        self.Smoke_with_HD = self.data_loader.data['Smoking'][self.data_loader.data['HeartDisease'] == 1]\n",
    "        self.Alcohol_with_HD = self.data_loader.data['AlcoholDrinking'][self.data_loader.data['HeartDisease'] == 1]\n",
    "        self.Stroke_with_HD = self.data_loader.data['Stroke'][self.data_loader.data['HeartDisease'] == 1]\n",
    "        self.PH_with_HD = self.data_loader.data['PhysicalHealth'][self.data_loader.data['HeartDisease'] == 1]\n",
    "        self.MH_with_HD = self.data_loader.data['MentalHealth'][self.data_loader.data['HeartDisease'] == 1]\n",
    "        self.DW_with_HD = self.data_loader.data['DiffWalking'][self.data_loader.data['HeartDisease'] == 1]\n",
    "        self.Sex_with_HD = self.data_loader.data['Sex'][self.data_loader.data['HeartDisease'] == 1]\n",
    "        self.AC_with_HD = self.data_loader.data['AgeCategory'][self.data_loader.data['HeartDisease'] == 1]\n",
    "        self.Diabetic_with_HD = self.data_loader.data['Diabetic'][self.data_loader.data['HeartDisease'] == 1]\n",
    "        self.PA_with_HD = self.data_loader.data['PhysicalActivity'][self.data_loader.data['HeartDisease'] == 1]\n",
    "        self.GH_with_HD = self.data_loader.data['GenHealth'][self.data_loader.data['HeartDisease'] == 1]\n",
    "        self.ST_with_HD = self.data_loader.data['SleepTime'][self.data_loader.data['HeartDisease'] == 1]\n",
    "        self.Asthma_with_HD = self.data_loader.data['Asthma'][self.data_loader.data['HeartDisease'] == 1]\n",
    "        self.KD_with_HD = self.data_loader.data['KidneyDisease'][self.data_loader.data['HeartDisease'] == 1]\n",
    "        self.SC_with_HD = self.data_loader.data['SkinCancer'][self.data_loader.data['HeartDisease'] == 1]\n",
    "\n",
    "        # Column Data without Hearth Disease\n",
    "        self.BMI_without_HD = self.data_loader.data['BMI'][self.data_loader.data['HeartDisease'] == 0]\n",
    "        self.Smoke_without_HD = self.data_loader.data['Smoking'][self.data_loader.data['HeartDisease'] == 0]\n",
    "        self.Alcohol_without_HD = self.data_loader.data['AlcoholDrinking'][self.data_loader.data['HeartDisease'] == 0]\n",
    "        self.Stroke_without_HD = self.data_loader.data['Stroke'][self.data_loader.data['HeartDisease'] == 0]\n",
    "        self.PH_without_HD = self.data_loader.data['PhysicalHealth'][self.data_loader.data['HeartDisease'] == 0]\n",
    "        self.MH_without_HD = self.data_loader.data['MentalHealth'][self.data_loader.data['HeartDisease'] == 0]\n",
    "        self.DW_without_HD = self.data_loader.data['DiffWalking'][self.data_loader.data['HeartDisease'] == 0]\n",
    "        self.Sex_without_HD = self.data_loader.data['Sex'][self.data_loader.data['HeartDisease'] == 0]\n",
    "        self.AC_without_HD = self.data_loader.data['AgeCategory'][self.data_loader.data['HeartDisease'] == 0]\n",
    "        self.Diabetic_without_HD = self.data_loader.data['Diabetic'][self.data_loader.data['HeartDisease'] == 0]\n",
    "        self.PA_without_HD = self.data_loader.data['PhysicalActivity'][self.data_loader.data['HeartDisease'] == 0]\n",
    "        self.GH_without_HD = self.data_loader.data['GenHealth'][self.data_loader.data['HeartDisease'] == 0]\n",
    "        self.ST_without_HD = self.data_loader.data['SleepTime'][self.data_loader.data['HeartDisease'] == 0]\n",
    "        self.Asthma_without_HD = self.data_loader.data['Asthma'][self.data_loader.data['HeartDisease'] == 0]\n",
    "        self.KD_without_HD = self.data_loader.data['KidneyDisease'][self.data_loader.data['HeartDisease'] == 0]\n",
    "        self.SC_without_HD = self.data_loader.data['SkinCancer'][self.data_loader.data['HeartDisease'] == 0]\n",
    "\n",
    "        self.variable_names = ['BMI_with_HD', 'Smoke_with_HD', 'Alcohol_with_HD', 'Stroke_with_HD', 'PH_with_HD',\n",
    "                               'MH_with_HD', 'DW_with_HD',\n",
    "                               'Sex_with_HD', 'AC_with_HD', 'Diabetic_with_HD', 'PA_with_HD', 'GH_with_HD',\n",
    "                               'ST_with_HD',\n",
    "                               'Asthma_with_HD', 'KD_with_HD', 'SC_with_HD', 'BMI_without_HD', 'Smoke_without_HD',\n",
    "                               'Alcohol_without_HD',\n",
    "                               'Stroke_without_HD', 'PH_without_HD', 'MH_without_HD', 'DW_without_HD', 'Sex_without_HD',\n",
    "                               'AC_without_HD',\n",
    "                               'Diabetic_without_HD', 'PA_without_HD', 'GH_without_HD', 'ST_without_HD',\n",
    "                               'Asthma_without_HD', 'KD_without_HD', 'SC_without_HD']\n",
    "        self.data_samples = (self.BMI_with_HD, self.Smoke_with_HD, self.Alcohol_with_HD,\n",
    "                             self.Stroke_with_HD, self.PH_with_HD, self.MH_with_HD, self.DW_with_HD, self.Sex_with_HD,\n",
    "                             self.AC_with_HD, self.Diabetic_with_HD, self.PA_with_HD, self.GH_with_HD, self.ST_with_HD,\n",
    "                             self.Asthma_with_HD, self.KD_with_HD, self.SC_with_HD, self.BMI_without_HD,\n",
    "                             self.Smoke_without_HD, self.Alcohol_without_HD, self.Stroke_without_HD,\n",
    "                             self.PH_without_HD, self.MH_without_HD, self.DW_without_HD, self.Sex_without_HD,\n",
    "                             self.AC_without_HD, self.Diabetic_without_HD, self.PA_without_HD, self.GH_without_HD,\n",
    "                             self.ST_without_HD, self.Asthma_without_HD, self.KD_without_HD, self.SC_without_HD)\n",
    "\n",
    "        self.normal_distributed_variables_with_HD = []\n",
    "\n",
    "        self.normal_distributed_variables_without_HD = []\n",
    "\n",
    "        self.not_normal_distributed_variables_with_HD = []\n",
    "\n",
    "        self.not_normal_distributed_variables_without_HD = []\n",
    "\n",
    "    def _wilcoxon_ranksum_test(self, group1, group2):\n",
    "        \"\"\"\n",
    "        Perform Wilcoxon rank-sum test (Mann-Whitney U test) for two independent samples.\n",
    "\n",
    "        Parameters:\n",
    "        - group1: List or array-like object containing data for sample 1.\n",
    "        - group2: List or array-like object containing data for sample 2.\n",
    "\n",
    "        Returns:\n",
    "        - statistic: The calculated test statistic.\n",
    "        - p_value: The p-value associated with the test statistic.\n",
    "        \"\"\"\n",
    "        statistic, p_value = sms.stattools.stats.mannwhitneyu(group1, group2)\n",
    "\n",
    "        return statistic, p_value\n",
    "\n",
    "    def _unpaired_t_test(self, group1, group2):\n",
    "        \"\"\"\n",
    "        Perform unpaired t-test for two groups.\n",
    "\n",
    "        Parameters:\n",
    "        - group1: List or array-like object containing data for group 1.\n",
    "        - group2: List or array-like object containing data for group 2.\n",
    "\n",
    "        Returns:\n",
    "        - t_statistic: The calculated t-statistic.\n",
    "        - p_value: The p-value associated with the t-statistic.\n",
    "        \"\"\"\n",
    "        t_statistic, p_value = ttest_ind(group1, group2)\n",
    "        return t_statistic, p_value\n",
    "\n",
    "    def perform_tests(self):\n",
    "        \"\"\"\n",
    "        Perform hypothesis tests for the normal and not normal distributed variables.\n",
    "\n",
    "        Prints the results of the tests.\n",
    "        \"\"\"\n",
    "        print(\"\\nUnpaired T-test tests for the normal distributed variables:\")\n",
    "        # Iterate over the indices of the arrays of the normal distributed variables\n",
    "        for i in range(len(self.normal_distributed_variables_with_HD)):\n",
    "            # Perform Unpaired T-Test\n",
    "            t_stat, p_val = self._unpaired_t_test(self.normal_distributed_variables_with_HD[i],\n",
    "                                                  self.normal_distributed_variables_without_HD[i])\n",
    "\n",
    "            # Print the results\n",
    "            print(f\"\\nUnpaired T-test test between the array of \"\n",
    "                  f\"{self.normal_distributed_variables_with_HD[i].name} with HeartDisease and the array without : \")\n",
    "            print(\"t-statistic:\", t_stat)\n",
    "            print(\"p-value:\", p_val)\n",
    "\n",
    "        print(\"\\nWilcoxon rank-sum tests for the not normal distributed variables:\")\n",
    "        # Iterate over the indices of the arrays of the not normal distributed variables\n",
    "        for i in range(len(self.not_normal_distributed_variables_with_HD)):\n",
    "            # Perform Wilcoxon rank-sum test\n",
    "            statistic, p_value = self._wilcoxon_ranksum_test(self.not_normal_distributed_variables_with_HD[i],\n",
    "                                                             self.not_normal_distributed_variables_without_HD[i])\n",
    "\n",
    "            # Print the results\n",
    "            print(f\"\\nWilcoxon rank-sum test between the array of \"\n",
    "                  f\"{self.not_normal_distributed_variables_with_HD[i].name} with HeartDisease and the array without : \")\n",
    "            print(\"Test statistic:\", statistic)\n",
    "            print(\"p-value:\", p_value)\n",
    "\n",
    "    def qq_plots(self, distribution='norm'):\n",
    "        \"\"\"\n",
    "        Generate Q-Q plots for multiple data samples.\n",
    "\n",
    "        Parameters:\n",
    "        - distribution: String indicating the theoretical distribution to compare against. Default is 'norm' for normal\n",
    "        distribution.\n",
    "        \"\"\"\n",
    "        num_samples = len(self.data_samples)\n",
    "        num_rows = (num_samples + 1) // 2  # Calculate the number of rows for subplots\n",
    "        num_cols = 2 if num_samples > 1 else 1  # Ensure at least 1 column for subplots\n",
    "\n",
    "        # Adjust the height of the figure to fit all Q-Q plots without overlapping\n",
    "        fig_height = 6 * num_rows  # Adjust this value as needed\n",
    "        fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, fig_height))\n",
    "        axes = axes.flatten()  # Flatten axes if multiple subplots\n",
    "\n",
    "        for i, data in enumerate(self.data_samples):\n",
    "            ax = axes[i]\n",
    "            probplot(data, dist=distribution, plot=ax)\n",
    "            ax.set_title(f'Q-Q Plot ({distribution})')\n",
    "            ax.set_xlabel('Theoretical Quantiles')\n",
    "            ax.set_ylabel(self.variable_names[i])\n",
    "\n",
    "        # Adjust layout and show plots\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def test_normality(self):\n",
    "        \"\"\"\n",
    "        Test the normality of multiple data samples using Shapiro-Wilk test.\n",
    "\n",
    "        Parameters:\n",
    "        - variable_names: List with the names of the variables to be tested.\n",
    "        - data_samples: Variable number of 1D array-like objects representing the data samples.\n",
    "\n",
    "        Returns:\n",
    "        - results: Dictionary containing the test results for each data sample.\n",
    "                   The keys are the variable names and the values are a tuple (test_statistic, p_value) for\n",
    "                   Shapiro-Wilk test.\n",
    "        \"\"\"\n",
    "\n",
    "        print('\\nNormality Test:\\n')\n",
    "\n",
    "        results = {}\n",
    "        normality = []\n",
    "        for name, data in zip(self.variable_names, self.data_samples):\n",
    "            results[name] = shapiro(data)\n",
    "        for variable_name, shapiro_result in results.items():\n",
    "            print(f'{variable_name}:')\n",
    "            print(f'Shapiro-Wilk test - Test statistic: {shapiro_result.statistic}, p-value: {shapiro_result.pvalue}')\n",
    "\n",
    "            if shapiro_result.pvalue > 0.05:\n",
    "                normality.append(variable_name)\n",
    "\n",
    "        if normality:\n",
    "            print(\"\\nThis variables seem normally distributed:\", normality)\n",
    "        else:\n",
    "            print(\"\\nNo variable seems normally distributed.\")\n",
    "\n",
    "    def distribute_normality_data(self):\n",
    "        \"\"\"\n",
    "        Distributes the data samples into different lists based on their normality.\n",
    "\n",
    "        This method iterates over the variable names and data samples, and categorizes them into different lists\n",
    "        based on their normality. If the variable name is 'BMI_with_HD', the data sample is added to the\n",
    "        'normal_distributed_variables_with_HD' list. If the variable name is 'BMI_without_HD', the data sample is\n",
    "        added to the 'normal_distributed_variables_without_HD' list. If the variable name contains 'with_HD', the\n",
    "        data sample is added to the 'not_normal_distributed_variables_with_HD' list. If the variable name contains\n",
    "        'without_HD', the data sample is added to the 'not_normal_distributed_variables_without_HD' list.\n",
    "        \"\"\"\n",
    "        for variable_name, data_sample in zip(self.variable_names, self.data_samples):\n",
    "            if variable_name == 'BMI_with_HD':\n",
    "                self.normal_distributed_variables_with_HD.append(data_sample)\n",
    "            elif variable_name == 'BMI_without_HD':\n",
    "                self.normal_distributed_variables_without_HD.append(data_sample)\n",
    "            else:\n",
    "                if \"with_HD\" in variable_name:\n",
    "                    self.not_normal_distributed_variables_with_HD.append(data_sample)\n",
    "                if \"without_HD\" in variable_name:\n",
    "                    self.not_normal_distributed_variables_without_HD.append(data_sample)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aedfdffe6fca4a46",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class FeatureCreation:\n",
    "    \"\"\"\n",
    "    A class that contains methods to create various features based on the data_loader object.\n",
    "\n",
    "    Args:\n",
    "        data_loader (object): An object that loads the data.\n",
    "\n",
    "    Attributes:\n",
    "        data_loader (object): An object that loads the data.\n",
    "\n",
    "    Methods:\n",
    "        _bmi_class_feature: Creates a BMI class feature based on the BMI column of the data.\n",
    "        _sleep_class_feature: Creates a sleep class feature based on the SleepTime column of the data.\n",
    "        _bad_habits_score_feature: Creates a bad habits score feature based on the Smoking and AlcoholDrinking columns of the data.\n",
    "        _diseases_feature: Creates a diseases feature based on the KidneyDisease, Asthma, SkinCancer, and Diabetic columns of the data.\n",
    "        _poor_health_days_month: Creates a poor health days per month feature based on the MentalHealth and PhysicalHealth columns of the data.\n",
    "        _dangerous_age_stroke: Creates a dangerous stroke feature based on the Stroke and AgeCategory columns of the data.\n",
    "        _age_bmi_interaction_feature: Creates an age-BMI interaction feature based on the AgeCategory and BMI columns of the data.\n",
    "        _bmi_sleep_interaction_feature: Creates a BMI-sleep interaction feature based on the BMI and SleepTime columns of the data.\n",
    "        _age_gh_interaction_feature: Creates an age-general health interaction feature based on the AgeCategory and GenHealth columns of the data.\n",
    "        _age_sleep_interaction_feature: Creates an age-sleep interaction feature based on the AgeCategory and SleepTime columns of the data.\n",
    "        create_modified_features: Calls the _bmi_class_feature and _sleep_class_feature methods to create modified features.\n",
    "        create_joined_features: Calls the _bad_habits_score_feature, _diseases_feature, _poor_health_days_month, and _dangerous_age_stroke methods to create joined features.\n",
    "        create_interaction_features: Calls the _age_bmi_interaction_feature, _bmi_sleep_interaction_feature, _age_gh_interaction_feature, and _age_sleep_interaction_feature methods to create interaction features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_loader):\n",
    "        \"\"\"\n",
    "        Initialize the class with a data loader.\n",
    "\n",
    "        Args:\n",
    "            data_loader: The data loader object used to load data.\n",
    "        \"\"\"\n",
    "        self.data_loader = data_loader\n",
    "\n",
    "    def _bmi_class_feature(self):\n",
    "        \"\"\"\n",
    "        Creates a BMI class feature based on the BMI column of the data.\n",
    "        The BMI class is determined by the following conditions:\n",
    "        - BMI < 18.5: Class 1\n",
    "        - 18.5 <= BMI < 25: Class 2\n",
    "        - 25 <= BMI < 30: Class 3\n",
    "        - 30 <= BMI < 35: Class 4\n",
    "        - 35 <= BMI < 40: Class 5\n",
    "        - BMI >= 40: Class 6\n",
    "        The BMI class is stored in the \"BMIClass\" column of the data_loader object.\n",
    "        \"\"\"\n",
    "        bmi = self.data_loader.data[\"BMI\"]\n",
    "        condition = [bmi < 18.5, bmi < 25, bmi < 30, bmi < 35, bmi < 40, bmi >= 40]\n",
    "        choice = [1, 2, 3, 4, 5, 6]\n",
    "        self.data_loader.data[\"BMIClass\"] = np.select(condition, choice)\n",
    "        print(\"Created BMIClass feature\\n\")\n",
    "\n",
    "    def _sleep_class_feature(self):\n",
    "        \"\"\"\n",
    "        Creates a sleep class feature based on the SleepTime column of the data.\n",
    "        The sleep class is determined by the following conditions:\n",
    "        - SleepTime < 6: Class 1\n",
    "        - 6 <= SleepTime < 9: Class 2\n",
    "        - SleepTime >= 9: Class 3\n",
    "        The sleep class is stored in the \"SleepClass\" column of the data_loader object.\n",
    "        \"\"\"\n",
    "        sleep = self.data_loader.data[\"SleepTime\"]\n",
    "        condition = [sleep < 6, sleep < 9, sleep >= 9]\n",
    "        choice = [1, 2, 3]\n",
    "        self.data_loader.data[\"SleepClass\"] = np.select(condition, choice)\n",
    "        print(\"Created SleepClass feature\\n\")\n",
    "\n",
    "    def _bad_habits_score_feature(self):\n",
    "        \"\"\"\n",
    "        Creates a bad habits score feature based on the Smoking and AlcoholDrinking columns of the data.\n",
    "        The bad habits score is calculated by summing the values of the Smoking and AlcoholDrinking columns.\n",
    "        The bad habits score is stored in the \"BadHabitsScore\" column of the data_loader object.\n",
    "        \"\"\"\n",
    "        smoker = self.data_loader.data[\"Smoking\"]\n",
    "        alcohol = self.data_loader.data[\"AlcoholDrinking\"]\n",
    "        condition = (smoker + alcohol)\n",
    "        self.data_loader.data[\"BadHabitsScore\"] = condition\n",
    "        print(\"Created BadHabitsScore feature\\n\")\n",
    "\n",
    "    def _diseases_feature(self):\n",
    "        \"\"\"\n",
    "        Creates a diseases feature based on the KidneyDisease, Asthma, SkinCancer, and Diabetic columns of the data.\n",
    "        The diseases feature is calculated by summing the values of the KidneyDisease, Asthma, SkinCancer, and Diabetic columns.\n",
    "        The diseases feature is stored in the \"Diseases\" column of the data_loader object.\n",
    "        \"\"\"\n",
    "        kidney_disease = self.data_loader.data[\"KidneyDisease\"]\n",
    "        asthma = self.data_loader.data[\"Asthma\"]\n",
    "        skin_cancer = self.data_loader.data[\"SkinCancer\"]\n",
    "        diabetic = self.data_loader.data[\"Diabetic\"]\n",
    "        condition = (kidney_disease + asthma + skin_cancer + diabetic)\n",
    "        self.data_loader.data[\"Diseases\"] = condition\n",
    "        print(\"Created Diseases feature\\n\")\n",
    "\n",
    "    def _poor_health_days_month(self):\n",
    "        \"\"\"\n",
    "        Creates a poor health days per month feature based on the MentalHealth and PhysicalHealth columns of the data.\n",
    "        The poor health days per month is calculated by summing the values of the MentalHealth and PhysicalHealth columns and dividing by 30.\n",
    "        The poor health days per month feature is stored in the \"PoorHealthDaysMonth\" column of the data_loader object.\n",
    "        \"\"\"\n",
    "        mental_health = self.data_loader.data[\"MentalHealth\"]\n",
    "        physical_health = self.data_loader.data[\"PhysicalHealth\"]\n",
    "        condition = (mental_health + physical_health) / 30\n",
    "        self.data_loader.data[\"PoorHealthDaysMonth\"] = condition\n",
    "        print(\"Created PoorHealthDaysMonth feature\\n\")\n",
    "\n",
    "    def _dangerous_age_stroke(self):\n",
    "        \"\"\"\n",
    "        Creates a dangerous stroke feature based on the Stroke and AgeCategory columns of the data.\n",
    "        The dangerous stroke feature is determined by the following conditions:\n",
    "        - AgeCategory >= 10 and Stroke = 1: 1\n",
    "        - Otherwise: 0\n",
    "        The dangerous stroke feature is stored in the \"DangerousStroke\" column of the data_loader object.\n",
    "        \"\"\"\n",
    "        strokes = self.data_loader.data[\"Stroke\"]\n",
    "        ages = self.data_loader.data[\"AgeCategory\"]\n",
    "        conditions = []\n",
    "        for stroke, age in zip(strokes, ages):\n",
    "            if age >= 10 and stroke == 1:\n",
    "                condition = 1\n",
    "            else:\n",
    "                condition = 0\n",
    "            conditions.append(condition)\n",
    "        self.data_loader.data[\"DangerousStroke\"] = conditions\n",
    "        print(\"Created DangerousStroke feature\\n\")\n",
    "\n",
    "    def _age_bmi_interaction_feature(self):\n",
    "        \"\"\"\n",
    "        Creates an age-BMI interaction feature based on the AgeCategory and BMI columns of the data.\n",
    "        The age-BMI interaction feature is calculated by multiplying the values of the AgeCategory and BMI columns.\n",
    "        The age-BMI interaction feature is stored in the \"AgeBMI_Interaction\" column of the data_loader object.\n",
    "        \"\"\"\n",
    "        age = self.data_loader.data[\"AgeCategory\"]\n",
    "        bmi = self.data_loader.data[\"BMI\"]\n",
    "        condition = (age * bmi)\n",
    "        self.data_loader.data[\"AgeBMI_Interaction\"] = condition\n",
    "        print(\"Created AgeBMI_Interaction feature\\n\")\n",
    "\n",
    "    def _bmi_sleep_interaction_feature(self):\n",
    "        \"\"\"\n",
    "        Creates a BMI-sleep interaction feature based on the BMI and SleepTime columns of the data.\n",
    "        The BMI-sleep interaction feature is calculated by multiplying the values of the BMI and SleepTime columns.\n",
    "        The BMI-sleep interaction feature is stored in the \"BMISleep_Interaction\" column of the data_loader object.\n",
    "        \"\"\"\n",
    "        sleep = self.data_loader.data[\"SleepTime\"]\n",
    "        bmi = self.data_loader.data[\"BMI\"]\n",
    "        condition = (sleep * bmi)\n",
    "        self.data_loader.data[\"BMISleep_Interaction\"] = condition\n",
    "        print(\"Created BMISleep_Interaction feature\\n\")\n",
    "\n",
    "    def _age_gh_interaction_feature(self):\n",
    "        \"\"\"\n",
    "        Creates an age-general health interaction feature based on the AgeCategory and GenHealth columns of the data.\n",
    "        The age-general health interaction feature is calculated by multiplying the values of the AgeCategory and GenHealth columns.\n",
    "        The age-general health interaction feature is stored in the \"AgeHealth_Interaction\" column of the data_loader object.\n",
    "        \"\"\"\n",
    "        age = self.data_loader.data[\"AgeCategory\"]\n",
    "        general_health = self.data_loader.data[\"GenHealth\"]\n",
    "        condition = (age * general_health)\n",
    "        self.data_loader.data[\"AgeHealth_Interaction\"] = condition\n",
    "        print(\"Created AgeHealth_Interaction feature\\n\")\n",
    "\n",
    "    def _age_sleep_interaction_feature(self):\n",
    "        \"\"\"\n",
    "        Creates an age-sleep interaction feature based on the AgeCategory and SleepTime columns of the data.\n",
    "        The age-sleep interaction feature is calculated by multiplying the values of the AgeCategory and SleepTime columns.\n",
    "        The age-sleep interaction feature is stored in the \"AgeSleep_Interaction\" column of the data_loader object.\n",
    "        \"\"\"\n",
    "        age = self.data_loader.data[\"AgeCategory\"]\n",
    "        sleep = self.data_loader.data[\"SleepTime\"]\n",
    "        condition = (age * sleep)\n",
    "        self.data_loader.data[\"AgeSleep_Interaction\"] = condition\n",
    "        print(\"Created AgeSleep_Interaction feature\\n\")\n",
    "\n",
    "    def create_modified_features(self):\n",
    "\n",
    "        self._bmi_class_feature()\n",
    "        self._sleep_class_feature()\n",
    "\n",
    "    def create_joined_features(self):\n",
    "\n",
    "        self._bad_habits_score_feature()\n",
    "        self._diseases_feature()\n",
    "        self._poor_health_days_month()\n",
    "        self._dangerous_age_stroke()\n",
    "\n",
    "    def create_interaction_features(self):\n",
    "\n",
    "        self._age_bmi_interaction_feature()\n",
    "        self._bmi_sleep_interaction_feature()\n",
    "        self._age_gh_interaction_feature()\n",
    "        self._age_sleep_interaction_feature()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da0a40706e39b2aa",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class KNearestNeighbors:\n",
    "    def __init__(self, k, radius=75):\n",
    "        self.k = k\n",
    "        self.radius = radius\n",
    "        self.nbrs = None\n",
    "        self.y_train = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        # Initialize NearestNeighbors object with algorithm='ball_tree' for efficient nearest neighbor search\n",
    "        self.nbrs = NearestNeighbors(n_neighbors=self.k, algorithm='ball_tree').fit(X_train)\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def score(self, X_val, y_val):\n",
    "        # Perform radius search for each validation data point\n",
    "        _, indices = self.nbrs.radius_neighbors(X_val, self.radius)\n",
    "\n",
    "        correct_counts = 0\n",
    "        total_counts = len(X_val)\n",
    "\n",
    "        # Iterate through each validation data point and count correct predictions\n",
    "        for i in range(total_counts):\n",
    "            neighbor_labels = self.y_train[indices[i]]  # Get labels of neighbors within the radius\n",
    "            if len(neighbor_labels) >= self.k:  # Check if the number of neighbors is at least k\n",
    "                predicted_label = np.bincount(neighbor_labels).argmax()  # Predict label based on majority vote\n",
    "                if predicted_label == y_val[i]:  # Check if prediction matches the true label\n",
    "                    correct_counts += 1\n",
    "            else:\n",
    "                raise ValueError(f\"\\nNumber of neighbors found ({len(neighbor_labels)}) is less than k ({self.k}).\")\n",
    "\n",
    "        accuracy = correct_counts / total_counts  # Calculate accuracy\n",
    "        return accuracy\n",
    "\n",
    "    def predict(self, X_val):\n",
    "        if self.nbrs is None:\n",
    "            raise ValueError(\"Model has not been trained yet. Please call fit() before predict().\")\n",
    "\n",
    "        # Perform radius search for each validation data point\n",
    "        _, indices = self.nbrs.radius_neighbors(X_val, self.radius)\n",
    "\n",
    "        y_pred = []\n",
    "\n",
    "        # Iterate through each validation data point and make predictions\n",
    "        for i in range(len(X_val)):\n",
    "            neighbor_labels = self.y_train[indices[i]]  # Get labels of neighbors within the radius\n",
    "            if len(neighbor_labels) >= self.k:  # Check if the number of neighbors is at least k\n",
    "                predicted_label = np.bincount(neighbor_labels).argmax()  # Predict label based on majority vote\n",
    "                y_pred.append(predicted_label)\n",
    "            else:\n",
    "                raise ValueError(f\"\\nNumber of neighbors found ({len(neighbor_labels)}) is less than k ({self.k}).\")\n",
    "\n",
    "        return np.array(y_pred)"
   ],
   "id": "54fe376ed40d443f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ModelOptimization:\n",
    "    \"\"\"\n",
    "    Class for optimizing the parameters of different classifiers.\n",
    "\n",
    "    Parameters:\n",
    "        - X_train (array-like): Training data.\n",
    "        - y_train (array-like): Training labels.\n",
    "        - X_val (array-like): Validation data.\n",
    "        - y_val (array-like): Validation labels.\n",
    "\n",
    "    Attributes:\n",
    "\n",
    "    Methods:\n",
    "        optimize_knn: Optimizes the parameters for K-Nearest Neighbors classifier.\n",
    "        optimize_logistic_regression: Optimizes the parameters for Logistic Regression classifier.\n",
    "        optimize_decision_tree: Optimizes the parameters for Decision Tree classifier.\n",
    "        optimize_mlp: Optimizes the parameters for Multi-layer Perceptron (MLP) classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "\n",
    "    def optimize_knn(self, k_values):\n",
    "        \"\"\"\n",
    "        Optimizes the parameters for K-Nearest Neighbors classifier.\n",
    "\n",
    "        Parameters:\n",
    "            k_values (list): List of k values to try.\n",
    "\n",
    "        Returns:\n",
    "            int: Best k value.\n",
    "        \"\"\"\n",
    "\n",
    "        best_k = None\n",
    "        best_accuracy = -1\n",
    "\n",
    "        for k in k_values:\n",
    "\n",
    "            knn = KNearestNeighbors(k)\n",
    "            #knn = KNeighborsClassifier(n_neighbors=k)\n",
    "            knn.fit(self.X_train, self.y_train)\n",
    "            accuracy = knn.score(self.X_val, self.y_val)\n",
    "            print(f\"k = {k}, Accuracy = {accuracy}\")\n",
    "\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_k = k\n",
    "\n",
    "        print(\"Best k value:\", best_k)\n",
    "        print(\"Best accuracy:\", best_accuracy)\n",
    "        return best_k\n",
    "\n",
    "    def optimize_logistic_regression(self, C_values=(0.01, 0.1, 1.0, 10.0), penalty=('l1', 'l2')):\n",
    "        \"\"\"\n",
    "        Optimizes the parameters for Logistic Regression classifier.\n",
    "\n",
    "        Parameters:\n",
    "            C_values (tuple): Values to try for regularization parameter C. Default is (0.01, 0.1, 1.0, 10.0).\n",
    "            penalty (tuple): Penalty values to try. Default is ('l1', 'l2').\n",
    "\n",
    "        Returns:\n",
    "            tuple: Best parameters for Logistic Regression (C, penalty).\n",
    "        \"\"\"\n",
    "        best_accuracy = -1\n",
    "        best_c = None\n",
    "        best_penalty = None\n",
    "\n",
    "        for c in C_values:\n",
    "            for penalty_selected in penalty:\n",
    "\n",
    "                lr = LogisticRegression(C=c, penalty=penalty_selected, solver='saga', multi_class='auto', max_iter=1000)\n",
    "                lr.fit(self.X_train, self.y_train)\n",
    "                accuracy = lr.score(self.X_val, self.y_val)\n",
    "\n",
    "                print(f\"C = {c}, Penalty = {penalty_selected}, Accuracy = {accuracy}\")\n",
    "\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_c = c\n",
    "                    best_penalty = penalty_selected\n",
    "\n",
    "        print(\"Best c value:\", best_c)\n",
    "        print(\"Best penalty:\", best_penalty)\n",
    "        print(\"Best accuracy:\", best_accuracy)\n",
    "        return best_c, best_penalty\n",
    "\n",
    "    def optimize_decision_tree(self, max_depth_values=(None, 5, 10, 20)):\n",
    "        \"\"\"\n",
    "        Optimizes the parameters for Decision Tree classifier.\n",
    "\n",
    "        Parameters:\n",
    "            max_depth_values (tuple): Values to try for max depth. Default is (None, 5, 10, 20).\n",
    "\n",
    "        Returns:\n",
    "            int: Best max depth value.\n",
    "        \"\"\"\n",
    "        best_accuracy = -1\n",
    "        best_max_depth = None\n",
    "\n",
    "        for max_depth in max_depth_values:\n",
    "\n",
    "            dt = DecisionTreeClassifier(max_depth=max_depth)\n",
    "            dt.fit(self.X_train, self.y_train)\n",
    "            accuracy = dt.score(self.X_val, self.y_val)\n",
    "\n",
    "            print(f\"Max depth = {max_depth}, Accuracy = {accuracy}\")\n",
    "\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_max_depth = max_depth\n",
    "\n",
    "        print(\"Best max depth value:\", best_max_depth)\n",
    "        print(\"Best accuracy:\", best_accuracy)\n",
    "        return best_max_depth\n",
    "\n",
    "    def optimize_mlp(self, population_size=20, max_generations=50, layer_range=(1, 100), activation=('logistic', 'tanh')):\n",
    "        \"\"\"\n",
    "        Optimizes the parameters for Multi-layer Perceptron (MLP) classifier.\n",
    "\n",
    "        Parameters:\n",
    "            population_size (int): Number of individuals in the population. Default is 20.\n",
    "            max_generations (int): Maximum number of generations. Default is 50.\n",
    "            layer_range (tuple): Range of values to try for the number of neurons in the hidden layer. Default is (1, 100).\n",
    "            activation (tuple): Activation functions to try. Default is ('logistic', 'tanh').\n",
    "\n",
    "        Returns:\n",
    "            tuple: Best parameters for MLP (neurons, activation).\n",
    "        \"\"\"\n",
    "        # Initialize the population\n",
    "        population = []\n",
    "\n",
    "        for _ in range(population_size):\n",
    "\n",
    "            neurons = random.randint(*layer_range)\n",
    "            activation_selected = random.choice(activation)\n",
    "            population.append((neurons, activation_selected))\n",
    "\n",
    "        # Random evolutionary search\n",
    "        for generation in range(max_generations):\n",
    "\n",
    "            print(f\"Generation {generation + 1}/{max_generations}\")\n",
    "            new_population = []\n",
    "            for i, (neurons, activation_selected) in enumerate(population):\n",
    "\n",
    "                # Skip the first iteration from generation 1 onwards since it is the best elements foun on the previous iteration\n",
    "                if generation != 0 and i == 0:\n",
    "                    new_population.append((best_params, best_accuracy))\n",
    "                    continue\n",
    "\n",
    "                print(\"Generation \", generation, \" element \", i)\n",
    "                mlp = MLPClassifier(hidden_layer_sizes=(neurons,), activation=activation_selected, early_stopping=True)\n",
    "                mlp.fit(self.X_train, self.y_train)\n",
    "                accuracy = mlp.score(self.X_val, self.y_val)\n",
    "                new_population.append(((neurons, activation_selected), accuracy))\n",
    "                print(f\"Neurons: {neurons}, activation: {activation_selected}, Accuracy: {accuracy}\")\n",
    "\n",
    "            new_population.sort(key=lambda x: x[1], reverse=True)\n",
    "            # Keep only the best element (concept of elitism in genetic algorithm)\n",
    "            best_params, best_accuracy = new_population[0]\n",
    "            population = [best_params]\n",
    "\n",
    "            # Use the parameters of the best individual to bias the generation of new individuals\n",
    "            best_neurons, best_activation = best_params\n",
    "\n",
    "            # Break when only 2 individuals are left\n",
    "            if population_size <= 2:\n",
    "                break\n",
    "            population_size -= 1\n",
    "\n",
    "            # Generate the new population\n",
    "            for _ in range(1, population_size):\n",
    "                # Randomly generate neurons with a bias towards the best_neurons\n",
    "                neurons = random.randint(np.ceil(best_neurons - 20) + 1, best_neurons + 20)\n",
    "                # Randomly select activation function\n",
    "                activation_selected = random.choice(activation)\n",
    "                population.append((neurons, activation_selected))\n",
    "\n",
    "        print(\"Best MLP parameters:\", best_params)\n",
    "        print(\"Best accuracy:\", best_accuracy)\n",
    "        return best_params"
   ],
   "id": "161edb17c122020e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CrossValidator:\n",
    "    \"\"\"\n",
    "    Class for performing k-fold cross-validation on machine learning models.\n",
    "\n",
    "    Parameters:\n",
    "        k (int): Number of folds for cross-validation. Default is 5.\n",
    "\n",
    "    Attributes:\n",
    "        k (int): Number of folds for cross-validation.\n",
    "        kf (object): KFold object for splitting the data.\n",
    "        cm (object): ConfusionMatrix object for calculating confusion matrix.\n",
    "        accuracy_scores (list): List of accuracy scores for each fold.\n",
    "        sensitivity_scores (list): List of sensitivity scores for each fold.\n",
    "        specificity_scores (list): List of specificity scores for each fold.\n",
    "\n",
    "    Methods:\n",
    "        cross_validate: Performs k-fold cross-validation on the model.\n",
    "        evaluate_on_test_set: Evaluates the trained model on the test set.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k=5):\n",
    "\n",
    "        self.k = k\n",
    "        self.kf = KFold(n_splits=k, shuffle=True)\n",
    "        self.cm = None\n",
    "        self.accuracy_scores = []\n",
    "        self.sensitivity_scores = []\n",
    "        self.specificity_scores = []\n",
    "\n",
    "    def cross_validate(self, model, X, y):\n",
    "        \"\"\"\n",
    "        Performs k-fold cross-validation on the model.\n",
    "\n",
    "        Parameters:\n",
    "            model: Machine learning model.\n",
    "            X (array-like): Features.\n",
    "            y (array-like): Labels.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Average accuracy, sensitivity, and specificity scores.\n",
    "        \"\"\"\n",
    "\n",
    "        for train_index, val_index in self.kf.split(X):\n",
    "            X_train, X_val = X[train_index], X[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_val)\n",
    "\n",
    "            self.cm = ConfusionMatrix(actual_vector=list(y_val), predict_vector=list(y_pred))\n",
    "\n",
    "            self.accuracy_scores.append(accuracy_score(y_val, y_pred))\n",
    "            self.sensitivity_scores.append(float(self.cm.TPR_Macro))\n",
    "            self.specificity_scores.append(float(self.cm.TNR_Macro))\n",
    "\n",
    "        avg_accuracy = sum(self.accuracy_scores) / len(self.accuracy_scores)\n",
    "        avg_sensitivity = sum(self.sensitivity_scores) / len(self.sensitivity_scores)\n",
    "        avg_specificity = sum(self.specificity_scores) / len(self.specificity_scores)\n",
    "\n",
    "        return avg_accuracy, avg_sensitivity, avg_specificity\n",
    "\n",
    "    def evaluate_on_test_set(self, model, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluates the trained model on the test set.\n",
    "\n",
    "        Parameters:\n",
    "            model: Trained machine learning model.\n",
    "            X_test (array-like): Test features.\n",
    "            y_test (array-like): Test labels.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Accuracy, sensitivity, and specificity scores on the test set.\n",
    "        \"\"\"\n",
    "        y_pred = model.predict(X_test)\n",
    "        cm = ConfusionMatrix(actual_vector=list(y_test), predict_vector=list(y_pred))\n",
    "        accuracy = cm.Overall_ACC\n",
    "        sensitivity = cm.TPR_Macro\n",
    "        specificity = cm.TNR_Macro\n",
    "        return accuracy, sensitivity, specificity"
   ],
   "id": "b0aa23c4a64e08d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ModelBuilding:\n",
    "    \"\"\"\n",
    "    Class for building, optimizing and evaluating machine learning models.\n",
    "\n",
    "    Parameters:\n",
    "        X_train (array-like): Training data.\n",
    "        y_train (array-like): Training labels.\n",
    "        X_test (array-like): Test data.\n",
    "        y_test (array-like): Test labels.\n",
    "        X_val (array-like): Validation data.\n",
    "        y_val (array-like): Validation labels.\n",
    "        k (int): Number of folds for cross-validation. Default is 5.\n",
    "        save_all (bool): Flag to save all models. Default is True.\n",
    "\n",
    "    Attributes:\n",
    "        X_train (array-like): Training data.\n",
    "        y_train (array-like): Training labels.\n",
    "        X_test (array-like): Test data.\n",
    "        y_test (array-like): Test labels.\n",
    "        X_val (array-like): Validation data.\n",
    "        y_val (array-like): Validation labels.\n",
    "        k (int): Number of folds for cross-validation.\n",
    "        save_all (bool): Flag to save all models.\n",
    "        best_model (object): Best performing model.\n",
    "        best_model_name (str): Name of the best performing model.\n",
    "        best_params (tuple): Best parameters for the best performing model.\n",
    "        best_score (float): Best validation score.\n",
    "        best_model_changed (bool): Flag to track if the best model changed.\n",
    "        history (dict): Dictionary to store the validation scores of all models.\n",
    "\n",
    "    Methods:\n",
    "        build_models: Builds, optimizes and evaluates machine learning models.\n",
    "        evaluate_best_model: Evaluates the best model on the test set.\n",
    "        save_model: Saves the model to a file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, X_val, y_val, k=5, save_all=True):\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.k = k\n",
    "        self.save_all = save_all\n",
    "        self.best_model = None\n",
    "        self.best_model_name = None\n",
    "        self.best_params = None\n",
    "        self.best_score = -1\n",
    "        self.best_model_changed = False\n",
    "        self.history = {}\n",
    "\n",
    "    def build_models(self, model_name, models_dict):\n",
    "        \"\"\"\n",
    "        Builds, optimizes and evaluates machine learning models.\n",
    "\n",
    "        Parameters:\n",
    "            model_name (str): Name of the model to build.\n",
    "            models_dict (dict): Dictionary containing the models to build.\n",
    "\n",
    "        Returns:\n",
    "            dict: History of validation scores for all models.\n",
    "            float: Average accuracy during cross-validation.\n",
    "            float: Average sensitivity during cross-validation.\n",
    "            float: Average specificity during cross-validation.\n",
    "        \"\"\"\n",
    "\n",
    "        model_optimization = ModelOptimization(self.X_train, self.y_train, self.X_val, self.y_val)\n",
    "        print(\"\\nTraining\", model_name, \"model\")\n",
    "        for name, model_params in models_dict.items():\n",
    "            if model_name != name:\n",
    "                continue\n",
    "            model = model_params.pop('model')\n",
    "            model_params_check = {}\n",
    "\n",
    "            if model_name == \"KNN\":\n",
    "                k = model_optimization.optimize_knn(model_params['k'])\n",
    "                model_params_check['k'] = k\n",
    "                params = {'k': k}\n",
    "            elif model_name == \"LogisticRegression\":\n",
    "                lr_params = model_optimization.optimize_logistic_regression(**model_params)\n",
    "                model_params_check['C'] = lr_params[0]\n",
    "                model_params_check['penalty'] = lr_params[1]\n",
    "                params = lr_params\n",
    "            elif model_name == \"DecisionTree\":\n",
    "                dt_params = model_optimization.optimize_decision_tree(**model_params)\n",
    "                model_params_check['max_depth'] = dt_params\n",
    "                params = dt_params\n",
    "            elif model_name == \"MLP\":\n",
    "                mlp_params = model_optimization.optimize_mlp(**model_params)\n",
    "                model_params_check['hidden_layer_sizes'] = (mlp_params[0],)\n",
    "                model_params_check['activation'] = mlp_params[1]\n",
    "                params = mlp_params\n",
    "            else:\n",
    "                raise ValueError(\"Model type is not supported.\")\n",
    "\n",
    "            model_instance = model(**model_params_check)\n",
    "            model_instance.fit(self.X_train, self.y_train)\n",
    "            val_score = model_instance.score(self.X_val, self.y_val)\n",
    "            self.history[str(name)] = val_score\n",
    "\n",
    "            if val_score > self.best_score:\n",
    "                print(\"\\nNew best model found!\")\n",
    "                self.best_score = val_score\n",
    "                self.best_model = model_instance\n",
    "                self.best_model_name = name\n",
    "                self.best_params = params\n",
    "                self.best_model_checked = model\n",
    "                self.best_model_params_checked = model_params_check\n",
    "                self.best_model_changed = True  # Update flag when a new best model is found\n",
    "            else:\n",
    "                self.best_model_changed = False  # Reset flag if the best model didn't change\n",
    "\n",
    "            if self.save_all:\n",
    "                self.save_model(model_instance, name)\n",
    "\n",
    "        print(\"\\nOptimization finished, history:\\n\")\n",
    "        print(\"Model name\\t\\tAccuracy\")  # Print header\n",
    "        for model, accuracy in self.history.items():  # Print table rows\n",
    "            print(f\"{model}\\t\\t{accuracy}\")\n",
    "        print(\"\\nBest performing model:\", self.best_model_name)\n",
    "        print(\"Best validation score:\", self.best_score)\n",
    "        print(\"Best parameters:\", self.best_params)\n",
    "\n",
    "        if not self.save_all:\n",
    "            self.save_model(self.best_model, self.best_model_name)\n",
    "\n",
    "        # Perform cross-validation only if the best model changed\n",
    "        if self.best_model_changed:\n",
    "            self.kf_cv = CrossValidator(k=self.k)\n",
    "            self.avg_accuracy, self.avg_sensitivity, self.avg_specificity = self.kf_cv.cross_validate(\n",
    "                self.best_model_checked(**self.best_model_params_checked), self.X_train, self.y_train)\n",
    "\n",
    "            print(\"Average accuracy during cross-validation:\", self.avg_accuracy)\n",
    "            print(\"Average sensitivity during cross-validation:\", self.avg_sensitivity)\n",
    "            print(f\"Average specificity during cross-validation: {self.avg_specificity}\\n\")\n",
    "\n",
    "        return self.history, self.avg_accuracy, self.avg_sensitivity, self.avg_specificity\n",
    "\n",
    "    def evaluate_best_model(self):\n",
    "        \"\"\"\n",
    "        Evaluates the best model on the test set.\n",
    "\n",
    "        Returns:\n",
    "            float: Test set score.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"\\nEvaluating best model on test set!\")\n",
    "        test_score = self.best_model.score(self.X_test, self.y_test)\n",
    "        print(\"Test set score:\", test_score)\n",
    "\n",
    "    def save_model(self, model, filename):\n",
    "        \"\"\"\n",
    "        Saves the model to a file.\n",
    "\n",
    "        Parameters:\n",
    "            model: Trained machine learning model.\n",
    "            filename (str): Name of the file to save the model.\n",
    "        \"\"\"\n",
    "\n",
    "        folder_path = \"./models\"\n",
    "        full_path = os.path.join(folder_path, filename)\n",
    "        print(\"Saving model as\", filename)\n",
    "        joblib.dump(model, full_path)"
   ],
   "id": "4694afa208e62214"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class BaggingClassifier:\n",
    "    \"\"\"\n",
    "    A class for implementing the Bagging ensemble method with a base model.\n",
    "\n",
    "    Parameters:\n",
    "        base_model: Base machine learning model to use for bagging.\n",
    "        X_train (array-like): Training features.\n",
    "        y_train (array-like): Training labels.\n",
    "        X_test (array-like): Test features.\n",
    "        y_test (array-like): Test labels.\n",
    "        n_straps (int): Number of bootstrap samples. Default is 100.\n",
    "        k_fold (int): Number of folds for cross-validation. Default is 5.\n",
    "\n",
    "    Attributes:\n",
    "        base_model: Base machine learning model to use for bagging.\n",
    "        n_straps (int): Number of bootstrap samples.\n",
    "        k_fold (int): Number of folds for cross-validation.\n",
    "        models (list): List of trained models.\n",
    "        X_train (array-like): Training features.\n",
    "        y_train (array-like): Training labels.\n",
    "        X_test (array-like): Test features.\n",
    "        y_test (array-like): Test labels.\n",
    "        accuracy_scores (list): List of accuracy scores.\n",
    "        sensitivity_scores (list): List of sensitivity scores.\n",
    "        specificity_scores (list): List of specificity scores.\n",
    "        avg_accuracy (float): Average accuracy score.\n",
    "        avg_sensitivity (float): Average sensitivity score.\n",
    "        avg_specificity (float): Average specificity score.\n",
    "\n",
    "    Methods:\n",
    "        examine_bagging: Fits the bagging ensemble on the training data with k fold cross-validation.\n",
    "        predict: Predicts class labels for input data.\n",
    "        evaluate: Evaluates the bagging ensemble on test data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_model, X_train, y_train, X_test, y_test, n_straps=100, k_fold=5):\n",
    "\n",
    "        self.base_model = base_model\n",
    "        self.n_straps = n_straps\n",
    "        self.k_fold = k_fold\n",
    "        self.models = []\n",
    "        self.X_train, self.y_train = X_train, y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.accuracy_scores = []\n",
    "        self.sensitivity_scores = []\n",
    "        self.specificity_scores = []\n",
    "        self.avg_accuracy = None\n",
    "        self.avg_sensitivity = None\n",
    "        self.avg_specificity = None\n",
    "\n",
    "    def examine_bagging(self):\n",
    "        \"\"\"\n",
    "        Fits the bagging ensemble on the training data with k fold cross-validation.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Average accuracy, sensitivity, and specificity scores.\n",
    "        \"\"\"\n",
    "        kfold = KFold(n_splits=self.k_fold, shuffle=True)\n",
    "        for train_index, val_index in kfold.split(self.X_train):\n",
    "            # Generate n_straps samples and train the models for the current fold\n",
    "            self.models = []\n",
    "            for _ in range(self.n_straps):\n",
    "                # Create bootstrap sample with the available indices of the fold\n",
    "                bootstrap_indices = np.random.choice(train_index, size=len(self.X_train[train_index]), replace=True)\n",
    "                X_bootstrap = self.X_train[bootstrap_indices]\n",
    "                y_bootstrap = self.y_train[bootstrap_indices]\n",
    "\n",
    "                # Train base model on bootstrap sample\n",
    "                model = clone(self.base_model)\n",
    "                model.fit(X_bootstrap, y_bootstrap)\n",
    "                self.models.append(model)\n",
    "\n",
    "            y_pred = self.predict(self.X_train[val_index])\n",
    "\n",
    "            self.cm = ConfusionMatrix(actual_vector=list(self.y_train[val_index]), predict_vector=list(y_pred))\n",
    "            self.accuracy_scores.append(accuracy_score(self.y_train[val_index], y_pred))\n",
    "            print(self.cm)\n",
    "            self.sensitivity_scores.append(float(self.cm.TPR_Macro))\n",
    "            self.specificity_scores.append(float(self.cm.TNR_Macro))\n",
    "\n",
    "        self.avg_accuracy = sum(self.accuracy_scores) / len(self.accuracy_scores)\n",
    "        self.avg_sensitivity = sum(self.sensitivity_scores) / len(self.sensitivity_scores)\n",
    "        self.avg_specificity = sum(self.specificity_scores) / len(self.specificity_scores)\n",
    "\n",
    "        return self.avg_accuracy, self.avg_sensitivity, self.avg_specificity\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts class labels for input data.\n",
    "\n",
    "        Parameters:\n",
    "            X (array-like): Features.\n",
    "\n",
    "        Returns:\n",
    "            array-like: Predicted class labels.\n",
    "        \"\"\"\n",
    "        # Aggregate predictions from all models\n",
    "        predictions = np.zeros((len(X), self.n_straps))\n",
    "        for i, model in enumerate(self.models):\n",
    "            predictions[:, i] = model.predict(X)\n",
    "\n",
    "        # Use majority voting to determine final prediction\n",
    "        final_predictions = np.apply_along_axis(lambda x: np.bincount(x.astype(int)).argmax(), axis=1, arr=predictions)\n",
    "        return final_predictions\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluates the bagging ensemble on test data.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Average accuracy, sensitivity, and specificity scores.\n",
    "        \"\"\"\n",
    "        self.y_pred = self.predict(self.X_test)\n",
    "        self.cm = ConfusionMatrix(actual_vector=list(self.y_test), predict_vector=list(self.y_pred))\n",
    "        self.accuracy_scores = accuracy_score(self.y_test, self.y_pred)\n",
    "        self.sensitivity_scores = float(self.cm.TPR_Macro)\n",
    "        self.specificity_scores = float(self.cm.TNR_Macro)\n",
    "        print(\n",
    "            f\"Accuracy: {self.accuracy_scores}, Sensitivity: {self.sensitivity_scores}, Specificity: {self.specificity_scores}\")\n",
    "        return self.accuracy_scores, self.sensitivity_scores, self.specificity_scores"
   ],
   "id": "eb7e31c52a0cec24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class AdaBoostClassifier:\n",
    "    \"\"\"\n",
    "    A class for implementing the AdaBoost ensemble method with a base model.\n",
    "\n",
    "    Parameters:\n",
    "        best_model: Best model to use for AdaBoost.\n",
    "        X_train (array-like): Training features.\n",
    "        y_train (array-like): Training labels.\n",
    "        X_test (array-like): Test features.\n",
    "        y_test (array-like): Test labels.\n",
    "        builder (object): Builder object with the configurations of the model.\n",
    "        n_estimators (int): Number of weak learners. Default is 50.\n",
    "        learning_rate (float): Learning rate shrinks the contribution of each weak learner. Default is 1.0.\n",
    "        k_fold (int): Number of folds for cross-validation. Default is 5.\n",
    "\n",
    "    Attributes:\n",
    "        best_model: Best model to use for AdaBoost.\n",
    "        n_estimators (int): Number of weak learners.\n",
    "        learning_rate (float): Learning rate shrinks the contribution of each weak learner.\n",
    "        k_fold (int): Number of folds for cross-validation.\n",
    "        models (list): List of trained models.\n",
    "        X_train (array-like): Training features.\n",
    "        y_train (array-like): Training labels.\n",
    "        X_test (array-like): Test features.\n",
    "        y_test (array-like): Test labels.\n",
    "        accuracy_scores (list): List of accuracy scores.\n",
    "        sensitivity_scores (list): List of sensitivity scores.\n",
    "        specificity_scores (list): List of specificity scores.\n",
    "        builder (object): Builder object with the configurations of the model.\n",
    "        avg_accuracy (float): Average accuracy score.\n",
    "\n",
    "    Methods:\n",
    "        train_adaboost: Trains the AdaBoost ensemble on the training data with k fold cross-validation.\n",
    "        predict: Predicts class labels for input data.\n",
    "        evaluate: Evaluates the AdaBoost ensemble on test data.\n",
    "    \"\"\"\n",
    "    def __init__(self, best_model, X_train, y_train, X_test, y_test, builder, n_estimators=50, learning_rate=1.0, k_fold=5):\n",
    "\n",
    "        self.best_model = best_model\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.k_fold = k_fold\n",
    "        self.models = []\n",
    "        self.X_train, self.y_train = X_train, y_train\n",
    "        self.X_test, self.y_test = X_test, y_test\n",
    "        self.accuracy_scores = []\n",
    "        self.sensitivity_scores = []\n",
    "        self.specificity_scores = []\n",
    "        self.builder = builder\n",
    "        self.avg_accuracy = None\n",
    "\n",
    "    def train_adaboost(self):\n",
    "        \"\"\"\n",
    "        Trains the AdaBoost ensemble on the training data with k fold cross-validation.\n",
    "\n",
    "        Returns:\n",
    "            float: Average accuracy score.\n",
    "        \"\"\"\n",
    "        kfold = KFold(n_splits=self.k_fold, shuffle=True)\n",
    "        for train_index, val_index in kfold.split(self.X_train):\n",
    "            # Initialize weights\n",
    "            w = np.ones(len(train_index)) / len(train_index)\n",
    "\n",
    "            models = []\n",
    "            for level in range(self.n_estimators):\n",
    "                print(\"Level \", level, \" of \", self.n_estimators-1)\n",
    "                # Train base model with weighted samples\n",
    "\n",
    "                base_model = None\n",
    "\n",
    "                if (self.best_model == 'KNN'):\n",
    "\n",
    "                    base_model = KNeighborsClassifier(n_neighbors=self.builder.best_model_params_checked['n_neighbors'])\n",
    "\n",
    "                elif (self.best_model == 'LogisticRegression'):\n",
    "\n",
    "                    base_model = LogisticRegression(C=self.builder.best_model_params_checked['C'],\n",
    "                                                    penalty=self.builder.best_model_params_checked['penalty'])\n",
    "\n",
    "                elif (self.best_model == 'DecisionTree'):\n",
    "\n",
    "                    base_model = DecisionTreeClassifier(max_depth=self.builder.best_model_params_checked['max_depth'])\n",
    "\n",
    "                elif (self.best_model == 'MLP'):\n",
    "\n",
    "                    base_model = MLPClassifier(hidden_layer_sizes=self.builder.best_model_params_checked['hidden_layer_sizes'],\n",
    "                                            activation=self.builder.best_model_params_checked['activation'], early_stopping=True)\n",
    "                else:\n",
    "                    raise ValueError(\"Best model not found.\")\n",
    "\n",
    "                base_model.fit(self.X_train[train_index], self.y_train[train_index], sample_weight=w)\n",
    "\n",
    "                print(\"Examining model\")\n",
    "                # Compute error\n",
    "                y_pred = base_model.predict(self.X_train[train_index])\n",
    "                err = np.sum(w * (y_pred != self.y_train[train_index])) / np.sum(w)\n",
    "\n",
    "                print(\"Updating Adaboost model\")\n",
    "                # Compute alpha\n",
    "                alpha = self.learning_rate * np.log((1 - err) / err)\n",
    "\n",
    "                # Update weights\n",
    "                w *= np.exp(alpha * (y_pred != self.y_train[train_index]))\n",
    "\n",
    "                models.append((base_model, alpha))\n",
    "\n",
    "            self.models.append(models)\n",
    "\n",
    "            # Compute predictions for validation set\n",
    "            y_pred = self.predict(val_index)\n",
    "\n",
    "            # Evaluate performance\n",
    "            self.accuracy_scores.append(accuracy_score(self.y_train[val_index], y_pred))\n",
    "            print(\"Accuracy:\", self.accuracy_scores[-1])\n",
    "\n",
    "        self.avg_accuracy = sum(self.accuracy_scores) / len(self.accuracy_scores)\n",
    "\n",
    "        return self.avg_accuracy\n",
    "\n",
    "    def predict(self, index):\n",
    "        \"\"\"\n",
    "        Predicts class labels for input data.\n",
    "\n",
    "        Parameters:\n",
    "            index (array-like): Indices of the data to predict.\n",
    "\n",
    "        Returns:\n",
    "            array-like: Predicted class labels.\n",
    "        \"\"\"\n",
    "        num_classes = len(np.unique(self.y_train))\n",
    "        predictions = np.zeros((len(index), num_classes))\n",
    "        for model, alpha in self.models[-1]:\n",
    "            y_pred = model.predict(self.X_train[index])\n",
    "            # Adjust the size of predictions array to match the expected size\n",
    "            temp_predictions = np.zeros((len(index), num_classes))\n",
    "            # Iterate over each sample and increment the corresponding class prediction\n",
    "            for i, pred in enumerate(y_pred):\n",
    "                temp_predictions[i, int(pred)] += alpha\n",
    "            predictions += temp_predictions\n",
    "\n",
    "        final_predictions = np.argmax(predictions, axis=1)\n",
    "        return final_predictions\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluates the AdaBoost ensemble on test data.\n",
    "\n",
    "        Returns:\n",
    "            float: Accuracy score.\n",
    "        \"\"\"\n",
    "        num_classes = len(np.unique(self.y_train))\n",
    "        predictions = np.zeros((len(self.y_test), num_classes))\n",
    "        for model, alpha in self.models[-1]:\n",
    "            y_pred = model.predict(self.X_test)\n",
    "            temp_predictions = np.zeros((len(self.y_test), num_classes))\n",
    "            for i, pred in enumerate(y_pred):\n",
    "                temp_predictions[i, int(pred)] += alpha\n",
    "            predictions += temp_predictions\n",
    "        final_predictions = np.argmax(predictions, axis=1)\n",
    "        self.accuracy_scores = accuracy_score(self.y_test, final_predictions)\n",
    "        print(f\"\\nEvaluated Accuracy: {self.accuracy_scores}\")\n",
    "        return self.accuracy_scores"
   ],
   "id": "e3ddd36f35bbe6ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CNN:\n",
    "    \"\"\"\n",
    "    A class to implement a Convolutional Neural Network (CNN) for binary classification.\n",
    "\n",
    "    Parameters:\n",
    "        X_train (array-like): Training data.\n",
    "        y_train (array-like): Training labels.\n",
    "        X_test (array-like): Test data.\n",
    "        y_test (array-like): Test labels.\n",
    "        X_val (array-like): Validation data.\n",
    "        y_val (array-like): Validation labels.\n",
    "        input_shape (tuple): Shape of the input data.\n",
    "        num_classes (int): Number of classes.\n",
    "        epochs (int): Number of epochs. Default is 10.\n",
    "        batch_size (int): Batch size. Default is 32.\n",
    "\n",
    "    Attributes:\n",
    "        X_train (array-like): Training data.\n",
    "        y_train (array-like): Training labels.\n",
    "        X_test (array-like): Test data.\n",
    "        y_test (array-like): Test labels.\n",
    "        X_val (array-like): Validation data.\n",
    "        y_val (array-like): Validation labels.\n",
    "        input_shape (tuple): Shape of the input data.\n",
    "        num_classes (int): Number of classes.\n",
    "        epochs (int): Number of epochs.\n",
    "        batch_size (int): Batch size.\n",
    "        model (object): CNN model.\n",
    "\n",
    "    Methods:\n",
    "        build_model: Builds the CNN model.\n",
    "        train: Trains the CNN model.\n",
    "        evaluate: Evaluates the CNN model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, X_val, y_val, input_shape, num_classes, epochs=10, batch_size=32):\n",
    "\n",
    "        self.X_train, self.y_train = X_train, y_train\n",
    "        self.X_test, self.y_test = X_test, y_test\n",
    "        self.X_val, self.y_val = X_val, y_val\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the CNN model.\n",
    "\n",
    "        Returns:\n",
    "            object: CNN model.\n",
    "        \"\"\"\n",
    "\n",
    "        model = Sequential([\n",
    "            Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(self.input_shape[0], 1)),\n",
    "            MaxPooling1D(pool_size=2),\n",
    "            Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "            MaxPooling1D(pool_size=2),\n",
    "            LSTM(64, return_sequences=True),\n",
    "            Flatten(),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')  # 1 porque estamos a fazer classificação binária\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the CNN model.\n",
    "\n",
    "        Returns:\n",
    "            object: Training history.\n",
    "        \"\"\"\n",
    "\n",
    "        # Ensure target labels are one-dimensional\n",
    "        y_train = self.y_train.squeeze()\n",
    "        y_val = self.y_val.squeeze()\n",
    "\n",
    "        history = self.model.fit(self.X_train, y_train, batch_size=self.batch_size, epochs=self.epochs,\n",
    "                                 verbose=1, validation_data=(self.X_val, y_val))\n",
    "        return history\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluates the CNN model.\n",
    "\n",
    "        Returns:\n",
    "            list: Scores of the model.\n",
    "        \"\"\"\n",
    "\n",
    "        # Ensure target labels are one-dimensional\n",
    "        y_test = self.y_test.squeeze()\n",
    "\n",
    "        scores = self.model.evaluate(self.X_test, y_test, verbose=0)\n",
    "        return scores"
   ],
   "id": "969ec6cd9e06dca8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ClusteringModel:\n",
    "    \"\"\"\n",
    "    A class to perform various clustering algorithms and visualize their results.\n",
    "\n",
    "    Attributes:\n",
    "        data_train (DataFrame): The training dataset.\n",
    "        data_test (DataFrame): The testing dataset.\n",
    "        n_clusters (int): The number of clusters to form.\n",
    "        train_labels (array): Labels generated by clustering algorithms for training data.\n",
    "        test_labels (array): Labels generated by clustering algorithms for testing data.\n",
    "\n",
    "    Methods:\n",
    "        __init__(self, data_train, data_test, n_clusters):\n",
    "            Initializes ClusteringModel with the provided datasets and number of clusters.\n",
    "\n",
    "        hierarchical_clustering(self):\n",
    "            Performs hierarchical clustering and visualizes the dendrogram.\n",
    "\n",
    "        k_means(self):\n",
    "            Performs K-Means clustering and visualizes the clusters.\n",
    "\n",
    "        gaussian_mixture_model(self):\n",
    "            Performs Gaussian Mixture Model clustering and visualizes the clusters.\n",
    "\n",
    "        optics(self):\n",
    "            Performs OPTICS clustering and visualizes the clusters.\n",
    "\n",
    "        plot_clusters(self, data, labels):\n",
    "            Plots clusters in 2D using Principal Component Analysis.\n",
    "\n",
    "        perform_clustering(self):\n",
    "            Performs all clustering methods and visualizes their results.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_train, data_test, n_clusters):\n",
    "        self.data_train = data_train\n",
    "        self.data_test = data_test\n",
    "        self.n_clusters = n_clusters\n",
    "        self.train_labels = None\n",
    "        self.test_labels = None\n",
    "\n",
    "    def hierarchical_clustering(self):\n",
    "        print(\"\\nHierarchical Clustering:\")\n",
    "        # Random sample 1% (we have too much data to compute efficiently so we need to reduce it)\n",
    "        random_sample = self.data_test.sample(n=int(0.01 * len(self.data_test)), replace=False)\n",
    "        # Plot dendrogram\n",
    "        sch.dendrogram(sch.linkage(random_sample, method='ward'), color_threshold=30)\n",
    "        plt.title('Dendrogram')\n",
    "        plt.xlabel('Samples')\n",
    "        plt.ylabel('Distance')\n",
    "        plt.show()\n",
    "\n",
    "    def k_means(self):\n",
    "        print(\"\\nK-Means Clustering:\")\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters)\n",
    "        kmeans.fit_predict(self.data_train)\n",
    "        self.test_labels = kmeans.predict(self.data_test)\n",
    "        self.plot_clusters(self.data_test, self.test_labels)\n",
    "\n",
    "    def gaussian_mixture_model(self):\n",
    "        print(\"\\nGaussian Mixture Model:\")\n",
    "        # Random sample 1% (we have too much data to compute efficiently so we need to reduce it)\n",
    "        random_sample = self.data_train.sample(n=int(0.01 * len(self.data_train)), replace=False)\n",
    "        gmm = GaussianMixture(n_components=self.n_clusters)\n",
    "        gmm.fit_predict(random_sample)\n",
    "        self.test_labels = gmm.predict(self.data_test)\n",
    "        self.plot_clusters(self.data_test, self.test_labels)\n",
    "\n",
    "    def optics(self):\n",
    "        print(\"\\nOPTICS Clustering:\")\n",
    "        # Random sample 10% (we have too much data to compute efficiently so we need to reduce it)\n",
    "        random_sample = self.data_test.sample(n=int(0.1 * len(self.data_test)), replace=False)\n",
    "        optics = OPTICS(min_samples=3)\n",
    "        self.test_labels = optics.fit(random_sample)\n",
    "        self.plot_clusters(random_sample, self.test_labels.labels_)\n",
    "\n",
    "    def plot_clusters(self, data, labels):\n",
    "        # Perform PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        data_pca = pca.fit_transform(data)\n",
    "\n",
    "        # Plot clusters in 2D\n",
    "        sns.scatterplot(x=data_pca[:, 0], y=data_pca[:, 1], hue=labels, palette='viridis')\n",
    "        plt.title(\"Clusters\")\n",
    "        plt.xlabel(\"Principal Component 1\")\n",
    "        plt.ylabel(\"Principal Component 2\")\n",
    "        plt.show()\n",
    "\n",
    "    def perform_clustering(self):\n",
    "        self.hierarchical_clustering()\n",
    "        self.k_means()\n",
    "        self.gaussian_mixture_model()\n",
    "        self.optics()"
   ],
   "id": "beafcabcf6f89e22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "247ec616096fb199"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
